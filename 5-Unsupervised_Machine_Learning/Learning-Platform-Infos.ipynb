{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information from our learning platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.1 Welcome to Moosic**\n",
    "\n",
    "As usual, the first page explains a bit background to determine the scope of our project:\n",
    "\n",
    "- Moosic is creating curated playlists, users like they refer to \"mood\" or \"style\"\n",
    "- We have cluster songs via audio features(wempo, energy,danceability,...)\n",
    "\n",
    "### ultimate question will be: **Can this clustering replace human curatoring?**\n",
    "\n",
    "* **Are Spotify’s audio features able to identify “similar songs”, as defined by humanly detectable criteria?** When you listen to two rock ballads, two operas or two drum & bass songs, you identify them as similar songs. Are these similarities detectable using the audio features from Spotify?\n",
    "* **Is K-Means a good method to create playlists?** Would you stick with this algorithm moving forward, or explore other methods to create playlists? \n",
    "\n",
    "Deadline: Friday."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2 Finding similar songs the human way**\n",
    "\n",
    "Examples given:\n",
    "- there might be \"a stark difference between two songs\" because of the language.\n",
    "- length of song obviously not a good measure\n",
    "\n",
    "-> Check \"descriptions\" in .csv file!!!\n",
    "\n",
    "\n",
    "\n",
    "**acousticness** - A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. - *Float*  \n",
    "**danceability** - Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\t- *Float*  \n",
    "**duration_ms** - The duration of the track in milliseconds. - *Integer*  \n",
    "**energy** - Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. - *Float*  \n",
    "**instrumentalness** - Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. - *Float*  \n",
    "**key** - The key the track is in. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. - *Integer*  \n",
    "**liveness** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. - *Float*  \n",
    "**loudness** - The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db. - *Float*  \n",
    "**mode** - Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. - *Integer*  \n",
    "**speechiness** - Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. - *Float*  \n",
    "**tempo** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. - *Float*  \n",
    "**time_signature** - An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). - *Integer*  \n",
    "**valence** - A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\t*Float*  \n",
    "\n",
    "\n",
    "To get a grasp on these, we got \"df_audio_features_10\" - to hear those: https://open.spotify.com/playlist/2WyxU9mT4S1XhBnQva7Jkt?si=2ef3cbca30aa4661&nd=1\n",
    "\n",
    "- A song like The Real Slim Shady by Eminem has a danceability value of almost 1 (the maximum), while a piano song by Brahms has an energy of 0.18. Do these extremes make sense while listening to the songs?\n",
    "- In Silence, by Amelie Lens, has a danceability value that is almost equal than Sultans of Swing by the Dire Straits. Why do you think that this feature is not able to capture the difference between these songs? Are there other features able to capture the difference? Which ones?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.3 Finding similar songs the machine way**\n",
    "\n",
    "*\"maths doesn’t always have to obey reality\"*:\n",
    "\"Even with an extra dimension, the maths stays pretty much the same, we just need to add an extra letter for the new dimension: a² + b² + c² = d²\" - and so on...\n",
    "\n",
    "Euclidean distance: \"Plain\" distance between two points.\n",
    "Manhattan distance: The distance between two points measured along axes at right angles.\n",
    "\n",
    "The task in python:  \n",
    "- Compute the Euclidean and Manhattan distances between all of the songs.  \n",
    "- Look at whether the distances between each pair of songs match your perceived differences.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.4 Intro to sklearn: transform your data with Transformers**\n",
    "\n",
    "We have to scale our data to calculate similarity, therefore using SKLearn. Code example from LMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. read the csv data into a Pandas DataFrame\n",
    "import pandas as pd\n",
    "df_audio_features = pd.read_csv(\"df_audio_features.csv\",\n",
    "                                index_col=[\"song_name\", \"artist\"])\n",
    " \n",
    "# 1. import the transformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# 2. initialize the transformer (optionally, set parameters)\n",
    "my_min_max = MinMaxScaler(feature_range=(0,1))\n",
    " \n",
    "# 3. fit the transformer to the data\n",
    "my_min_max.fit(df_audio_features)\n",
    " \n",
    "# 4. use the transformer to transform the data\n",
    "scaled_audio_features = my_min_max.transform(df_audio_features)\n",
    " \n",
    "# 5. reconvert the transformed data back to a DataFrame\n",
    "pd.DataFrame(scaled_audio_features,\n",
    "             index=df_audio_features.index,\n",
    "             columns=df_audio_features.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding that example:\n",
    "\n",
    "1. Import only needed parts for the sake of speed!\n",
    "2. Transformers are its own class of object with its own method!\n",
    "3. Calculations to \"fit\" the raw data.\n",
    "4. Storing into a new object.\n",
    "5. Recovering original names from dataframe, not needed but useful for exploration.\n",
    "   \n",
    "steps 3-5 together = fit_transform()\n",
    "\n",
    "\n",
    "For the \"df_audio_features_1000.csv\"(copied from LMS):\n",
    "\n",
    "* Use MinMaxScaler() to scale the data.\n",
    "* Use Matplotlib or Pandas to plot the distributions of all the variables. Are the distributions skewed? Are there outliers?\n",
    "* Try using a different transformer to scale the data. Read the documentation learn what they do (links below), and plot the distributions of the features after each transformation to explore how the data looks like. These are some options:\n",
    "    * StandardScaler()\n",
    "    * RobustScaler()\n",
    "    * QuantileTransformer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.5 Apply K-means to create clusters of songs**\n",
    "\n",
    "Stat Quest video on the algorithm: https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer\n",
    "Diving deeper on the topic: https://www.youtube.com/watch?v=_aWzGGNrcic&ab_channel=VictorLavrenko\n",
    "\n",
    "\n",
    "Process of using a Model(LMS):\n",
    "\n",
    "1. Import the model from the pertinent submodule, in this case sklearn.cluster.\n",
    "2. Initialize the model with the parameters (for now, we will set the n_clusters parameter to 4, we will worry about this later).\n",
    "3. Fit the model to the data using its fit method.\n",
    "4. Obtain the cluster for each observation with the predict method (we are not actually making a “prediction” in the common meaning of the word, but this is how all models in Scikit-Learn give their main output, and for a clustering algorithm this is the clusters).\n",
    "5. Attach the cluster output to our original DataFrame, so that we can explore which cluster corresponds to which song.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as in 6.4, from LMS:\n",
    "\n",
    "# 1. import the model\n",
    "from sklearn.cluster import KMeans\n",
    " \n",
    "# 2. initialize the model\n",
    "my_kmeans = KMeans(n_clusters= 4)\n",
    " \n",
    "# 3. fit the model to the data\n",
    "my_kmeans.fit(your_data) # pass your scaled data here\n",
    " \n",
    "# 4. obtain the cluster output\n",
    "clusters = my_kmeans.predict(your_data) # pass your scaled data here\n",
    " \n",
    "# 5. attach the cluster output to our original DataFrame\n",
    "your_original_dataframe[\"cluster\"] = clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look into the dataframe now; turns out to be clusters, leading to:\n",
    "\n",
    "\t\"df_audio_features.groupby(by=\"cluster\").mean()\"\n",
    "\n",
    "similar to the centroids:\n",
    "\n",
    "\t\"centroids = my_kmeans.cluster_centers_\"\n",
    "\t\"pd.DataFrame(centroids)\"\n",
    "\n",
    "\n",
    "Other interesting tasks you might want to perform in terms of cluster exploration include:\n",
    "\n",
    "* Looking at the size of the clusters (how many observations do they have?)\n",
    "* Plotting the distribution of an individual feature (such as “energy”) across different clusters.\n",
    "* Visualizing the clusters in a scatterplot with two different features.\n",
    "* Listening to a few songs from each cluster to get a feeling for what do these clusters contain.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.6 Choose the right number of clusters**\n",
    "\n",
    "**Business requirements**: you have been asked to keep the size of all the playlists at between 50 and 250 songs. If you have a dataset with roughly 1500 songs, that means that you should explore having between 6 and 30 clusters.\n",
    "**Technical methods**: From the mathematical point of view, a good clustering model is the one in which all the clusters are very different from each other, while all the observations of each cluster are very similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regarding inertia:\n",
    "from sklearn.cluster import KMeans\n",
    "myKMeans = KMeans(n_clusters=2)\n",
    "myKMeans.fit(my_data)\n",
    "myKMeans.inertia_\n",
    "#for loop to compute the clustering for each value and append the inertia into a list:\n",
    "inertia_list = []\n",
    "for i in range(1,30):\n",
    "    myKMeans = KMeans(n_clusters=i)\n",
    "    myKMeans.fit(scaled_audio_features)\n",
    "    inertia_list.append(round(myKMeans.inertia_))\n",
    "#silhouette coefficient:\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(your_data, your_kmeans.labels_)\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette coefficient:\n",
    "For every observation in a clustering algorithm, we can look at these two metrics:\n",
    "* a is the mean distance to the other instances in the same cluster\n",
    "* b is the mean distance to the instances of the next closest cluster\n",
    "\n",
    "Your **tasks**, moving on with the project, are the following ones:\n",
    "\n",
    "* Use the metrics discussed in this lesson, as well as your own judgement in terms of business adequacy, to choose and justify the right number of clusters.\n",
    "* Run your Exploratory Analysis again with the number of clusters that you have finally chosen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.7 Clustering: second iteration**\n",
    "\n",
    "1. Data preparation:\n",
    "    * Reading the data\n",
    "    * Initial quick exploration\n",
    "    * Dropping unwanted features\n",
    "2. Modelling:\n",
    "    * Data scaling (potentially, other transformations)\n",
    "    * K-Means exploration of clusters (elbow method, silhouette coefficient…)\n",
    "    * K-Means final model\n",
    "3. Cluster exploration:\n",
    "    * Univariate and bivariate exploration of the clusters\n",
    "    * Manual labelling of the clusters\n",
    "\n",
    "\n",
    "* Clean and comment your code. Discard unused code and describe the process on markdown cells.\n",
    "* Structure properly your directory, with the proper folders and no “Untitled.ipynb” files floating around.\n",
    "* When possible, create functions and import them into your notebooks.\n",
    "* Unit-test your scripts and notebooks by making sure they are bug-free when you run them from scratch.\n",
    "\n",
    "-> Second iteration with 5000 songs df!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.8 Prepare your clustering presentation**\n",
    "\n",
    "* Are Spotify’s audio features able to identify “similar songs”, as defined by humanly detectable criteria? When you listen to two rock ballads, two operas or two drum & bass songs, you identify them as similar songs. Are these similarities detectable using the audio features from Spotify?\n",
    "* Is K-Means a good method to create playlists? Would you stick with this algorithm moving forward, or explore other methods to create playlists?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea1167cbcaad81cdbbc35255037ba90d13cf758e6bff12ff9eb1557202ecbe5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
